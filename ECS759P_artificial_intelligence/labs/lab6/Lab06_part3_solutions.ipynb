{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT2CO3WQ_uZc"
   },
   "source": [
    "# ECS759P Lab6 Part 3: Neural networks for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg0zqfEK9-Mu"
   },
   "source": [
    "## Data\n",
    "We are going to use in this lab the MNIST dataset containing images (28x28 pixels) of hand written digits. `PyTorch` provides an API to get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ukiRwCrB-A9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "validation_dataset = datasets.MNIST('./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VEXUB_rCVVT"
   },
   "source": [
    "Now that the data downloaded, let's create a data loader which will allow you to access images and their corresponding labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4i05yzgCCsOe"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=35, shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=35, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F758ChaZDAXi"
   },
   "source": [
    "Let's now have a look at the data we are dealing with as well as its dimension (you can run the cell several times to see what happens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZ7ggePbDLdK"
   },
   "outputs": [],
   "source": [
    "input_data, label = next(iter(train_loader))\n",
    "plt.imshow(input_data[0,:,:,:].numpy().reshape(28,28), cmap=\"gray_r\");\n",
    "print(\"Label is: {}\".format(label[0]))\n",
    "print(\"Dimension of input data: {}\".format(input_data.size()))\n",
    "print(\"Dimension of labels: {}\".format(label.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfdgebsdbNaK"
   },
   "source": [
    "Data loaders add additional functionalities (like picking batches, shuffling, etc).\n",
    "\n",
    "There are many pre-made data loaders like MNIST, but there is also a possibility of creating custom dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le3OdFbDbi7H"
   },
   "source": [
    "## Deep Network\n",
    "\n",
    "**Q. By filling the gaps below, implement a Multi-layer Neural Network with several hidden layers (number to explore as a hyperparameter). Initially, we are only interested in using the sigmoid activation function and the `SGD` optimiser. Test your networks using the provided data loader and compute the accuracy on the validation set. What is the impact of the different hyperparameters: number of epochs, learning rate $\\eta$, initial weights and number of neurons of the hidden layers, number of hidden layers?**\n",
    "\n",
    "\n",
    "**A. First thing to note is that the classification accuracy is usually already good with only one hidden layer. More layers do not change the final performance too much. However, when adding more layers, we need more epochs since we have more weights to learn. Starting with only zeros or ones is usually worse than using some random-based distribution. It is quite difficult to make any distinct conclusion regarding the impact of the number of neurons for each layer on the performance. A very low learning rate will slow down the convergence whereas a too high learning rate will result in a neural network that diverges and will get bad results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgbLBUhOdq9q"
   },
   "outputs": [],
   "source": [
    "# Definition of the neural network\n",
    "class MyMLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyMLP, self).__init__()\n",
    "    # TO DO\n",
    "    # This would be a single hidden layer neural network\n",
    "    self.fc = nn.Linear(28*28, 256)\n",
    "    self.act = nn.Sigmoid()\n",
    "    self.fc1 = nn.Linear(256, 10)\n",
    "\n",
    "    # Alternatively use the Sequential container to run layers sequentially\n",
    "    # self.fc_model = nn.Sequential(nn.Linear(28*28, 256), nn.Sigmoid(), nn.Linear(256,10))\n",
    "\n",
    "    # This would be a neural net with 3 hidden layers and sigmoid function\n",
    "    # self.fc_model = nn.Sequential(nn.Linear(28*28, 1046), nn.Sigmoid(), nn.Linear(1046,512), nn.Sigmoid(), nn.Linear(512, 256), nn.Sigmoid(), nn.Linear(256, 10))\n",
    "    # This would be another neural net with 2 hidden layers and ReLU function\n",
    "    #self.fc_model = nn.Sequential(nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512,256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # TO DO\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.fc(x)\n",
    "    x = self.act(x)\n",
    "    x = self.fc1(x)\n",
    "    # Alternatively use the Sequential container to run layers sequentially\n",
    "    # x = self.fc_model(x)\n",
    "    return x\n",
    "\n",
    "def evaluation(dataloader):\n",
    "  total, correct = 0,0\n",
    "  net.eval()\n",
    "  # TO DO\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net(inputs)\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (pred == labels).sum().item()\n",
    "  return 100 * correct / total\n",
    "\n",
    "def weights_init(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "      # TO DO\n",
    "      # This is a gaussian initialization following N(0.5,2)\n",
    "      nn.init.normal_(layer.weight, 0.5, 2)\n",
    "      # This would be an uniform initialization between -1 and 1\n",
    "      # nn.init.uniform_(layer.weight, -1, 1)\n",
    "      # This would be an initialization with only zeros\n",
    "      # nn.init.zeros_(layer.weight)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = MyMLP().to(device)\n",
    "net.apply(weights_init)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# SGD optimizer with learning rate of 0.1\n",
    "opt = torch.optim.SGD(list(net.parameters()), lr = 0.1)\n",
    "# This would be for Adam optimizer with learning rate = 0.05\n",
    "# opt = torch.optim.Adam(list(net.parameters()), lr = 0.05)\n",
    "# This would be SGD with Momentum with a lr=0.025 and a momentum of 0.9\n",
    "# opt = torch.optim.SGD(list(net.parameters()), lr = 0.025, momentum = 0.025)\n",
    "\n",
    "\n",
    "# Training phase\n",
    "\n",
    "# Change this value to make training longer\n",
    "max_epochs = 5\n",
    "loss_epoch_array = []\n",
    "loss_epoch = 0\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "for epoch in range(max_epochs):\n",
    "  loss_epoch = 0\n",
    "  print(epoch)\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    # TO DO\n",
    "    net.train()\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    opt.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loss_epoch += loss.item()\n",
    "    # TO DO\n",
    "  loss_epoch_array.append(loss_epoch)\n",
    "  train_accuracy.append(evaluation(train_loader))\n",
    "  valid_accuracy.append(evaluation(validation_loader))\n",
    "  print(\"Epoch {}: loss: {}, train accuracy: {}, valid accuracy:{}\".format(epoch + 1, loss_epoch_array[-1], train_accuracy[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twDWegPkdtUB"
   },
   "source": [
    "\n",
    "**Q. Test other activation functions in your Deep Network, especially  `ReLU()`. Briefly explain what effect you observe on the performance.**\n",
    "\n",
    "**A. Using ReLU does not seem to change significantly the results with only a few layers but its impact is getting more obvious when the network grows.**\n",
    "\n",
    "**Q. Use other optimizers such as `SGD with momentum` and `Adam`. What do you observe during the learning phase?**\n",
    "\n",
    "**A. The loss change pattern is different. However, the results are not that different.**\n",
    "\n",
    "**Q. When dealing with images, we usually prefer using Convolutional Neural Networks (CNN). By filling the gaps below, implement the LeNet-5 architecture (see lenet-5.png attached to the lab) using `PyTorch`. Do you see any improvement in terms of performance?**\n",
    "**A. The performance obtained using LeNet-5 is better than any MLP tried before.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYWt40a3DWg4"
   },
   "outputs": [],
   "source": [
    "# CNN implementation\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyCNN, self).__init__()\n",
    "    # TO DO\n",
    "    self.conv = nn.Conv2d(1, 6, kernel_size = 5)\n",
    "    self.act_conv = nn.Tanh()\n",
    "    self.avg_pool = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "    self.conv1 = nn.Conv2d(6, 16, kernel_size = 5)\n",
    "    self.act_conv1 = nn.Tanh()\n",
    "    self.avg_pool1 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "    # Alternatively use the Sequential container to run layers sequentially\n",
    "\n",
    "    # self.cnn_model = nn.Sequential(nn.Conv2d(1, 6, kernel_size = 5), nn.Tanh(), nn.AvgPool2d(2, stride=2), nn.Conv2d(6, 16, kernel_size = 5), nn.Tanh(), nn.AvgPool2d(2, stride = 2))\n",
    "\n",
    "    self.fc = nn.Linear(256, 120)\n",
    "    self.act = nn.Tanh()\n",
    "    self.fc1 = nn.Linear(120, 84)\n",
    "    self.act1 = nn.Tanh()\n",
    "    self.fc2 = nn.Linear(84, 10)\n",
    "\n",
    "    # Alternatively use the Sequential container to run layers sequentially\n",
    "\n",
    "    # self.fc_model = nn.Sequential(nn.Linear(256, 120), nn.Tanh(), nn.Linear(120,84), nn.Tanh(), nn.Linear(84, 10))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # TO DO\n",
    "\n",
    "    x = self.conv(x)\n",
    "    x = self.act_conv(x)\n",
    "    x = self.avg_pool(x)\n",
    "\n",
    "    x = self.conv1(x)\n",
    "    x = self.act_conv1(x)\n",
    "    x = self.avg_pool1(x)\n",
    "\n",
    "    x = x.view(x.size(0), -1)\n",
    "\n",
    "    x = self.fc(x)\n",
    "    x = self.act(x)\n",
    "    x = self.fc1(x)\n",
    "    x = self.act1(x)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    # Alternatively use the Sequential container to run layers sequentially\n",
    "\n",
    "    # x = self.cnn_model(x)\n",
    "    # x = x.view(x.size(0), -1)\n",
    "    # x = self.fc_model(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "net = MyCNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(list(net.parameters()))\n",
    "\n",
    "def evaluation(dataloader):\n",
    "  total, correct = 0,0\n",
    "  net.eval()\n",
    "  for data in dataloader:\n",
    "    # TO DO\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net(inputs)\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (pred == labels).sum().item()\n",
    "  return 100 * correct / total\n",
    "\n",
    "loss_epoch_array = []\n",
    "max_epochs = 50\n",
    "loss_epoch = 0\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "for epoch in range(max_epochs):\n",
    "  loss_epoch = 0\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    # TO DO\n",
    "    net.train()\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    opt.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loss_epoch += loss.item()\n",
    "    # TO DO\n",
    "  loss_epoch_array.append(loss_epoch)\n",
    "  train_accuracy.append(evaluation(train_loader))\n",
    "  valid_accuracy.append(evaluation(validation_loader))\n",
    "  print(\"Epoch {}: loss: {}, train accuracy: {}, valid accuracy:{}\".format(epoch + 1, loss_epoch_array[-1], train_accuracy[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
