{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c07R-sKwROm3"
   },
   "source": [
    "# ECS759P Lab3 - Informed and Local Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hohe3h_xNkad"
   },
   "source": [
    "## Recap\n",
    "\n",
    "Last week we went through examples of **uninformed** search using *DFS* and *BFS*. These algorithms are primarily concerned with finding **a** solution. The **uninformed** search algorithm that takes into account the path costs of the solution and find the minimum cost one is *UCS*. However, we are leaving that to your coursework 1.\n",
    "\n",
    "## Before you proceed\n",
    "\n",
    "Make sure you have uploaded the `middle_labyrinth.json` file to the same directory as this notebook. If you don't know how, ask for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CEI_-msROm7"
   },
   "source": [
    "# Informed Search with A*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vn1KxXakROm8"
   },
   "source": [
    "\n",
    "All of these search algorithms mentioned above, DFS, BFS and UCS are *uninformed* (*blind*). For some problems, you can *help* the search by introducing some side information based on the nature of the problem. We will investigate this idea by applying the __A*__ search to an example problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1XVz1NBROm9"
   },
   "source": [
    "## Moving in a labyrinth like a pro\n",
    "Let's try to solve the same problem as last week but with A\\*, an informed search algorithm. Just for reminder, A\\* is a search algorithm that uses a combination of a **cost** function and **admissible heuristics** (which is an estimation of the remaining cost to get to the destination) in order to find the best path in a more efficient way. Let's take for instance the following 10x10 labyrinth:\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1UX1EjbQl7kQ9EeW26cn1ArYHgUsAJWkR)\n",
    "\n",
    "Knowing that the initial state (or entrance) is encoded as the state `(0,0)` and the goal state (or exit) is encoded as `(9,9)`, can you come up with a simple heuristic allowing you to find the path faster? (**Try to think a bit before reading the next part**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1DnADknVW-y"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "\n",
    "def load_graph_from_file(filename):\n",
    "  with open(filename) as labyrinth_file:\n",
    "    dict_labyrinth = json.load(labyrinth_file)\n",
    "    return nx.Graph(dict_labyrinth)\n",
    "\n",
    "def show_weighted_graph(networkx_graph, node_size, font_size, fig_size):\n",
    "  # Allocate the given fig_size in order to have space for each node\n",
    "  plt.figure(num=None, figsize=fig_size, dpi=80)\n",
    "  plt.axis('off')\n",
    "  # Compute the position of each vertex in order to display it nicely\n",
    "  nodes_position = nx.spring_layout(networkx_graph)\n",
    "  # You can change the different layouts depending on your graph\n",
    "  # Extract the weights corresponding to each edge in the graph\n",
    "  edges_weights  = nx.get_edge_attributes(networkx_graph,'weight')\n",
    "  # Draw the nodes (you can change the color)\n",
    "  nx.draw_networkx_nodes(networkx_graph, nodes_position, node_size=node_size,\n",
    "                         node_color = [\"orange\"]*networkx_graph.number_of_nodes())\n",
    "  # Draw only the edges\n",
    "  nx.draw_networkx_edges(networkx_graph, nodes_position,\n",
    "                         edgelist=list(networkx_graph.edges), width=2)\n",
    "  # Add the weights\n",
    "  nx.draw_networkx_edge_labels(networkx_graph, nodes_position,\n",
    "                               edge_labels = edges_weights)\n",
    "  # Add the labels of the nodes\n",
    "  nx.draw_networkx_labels(networkx_graph, nodes_position, font_size=font_size,\n",
    "                          font_family='sans-serif')\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "middle_labyrinth = load_graph_from_file(\"middle_labyrinth.json\")\n",
    "show_weighted_graph(middle_labyrinth, 1000, 12, (20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lei4BzZJSm9r"
   },
   "source": [
    "Although different heuristics can help finding the path quicker, in order to be useful for A\\*, the heuristic needs to be **admissible** and, for graph search version, **consistent (monotonous)**:\n",
    "\n",
    "- **Admissible** means that it should *never over-estimate* the cost of reaching the goal.\n",
    "- **Consistent** means that if the heuristic cost of each node (which is an estimation of the cost to get to the goal from that node) is no more than the cost of getting to one of its successors plus the heuristic cost of that successor (refer to the triangle inequality).\n",
    "\n",
    "Finally, the heuristic cost on a goal node should be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n78ondcpROnA"
   },
   "source": [
    "Here, since we know the coordinates of all the cells, the idea would be to favour moving in the general direction of the goal compared to moving back.\n",
    "\n",
    "One way to quantify this is using the **Manhattan distance**  between a given cell and the exit cell as our heuristic.\n",
    "\n",
    "The Manhattan distance, (a.k.a. taxi-cab distance, or city-block distance, or L-1 distance) is defined as the (absolute) difference between the row indices of the current position and goal, plus the (absolute) difference between their column indices. So it is similar to the good-old-fashioned Euclidean distance, except that instead of squaring the differences of the elements, we use the absolute value of the differences of the elements.\n",
    "\n",
    "\n",
    "For instance the Manhattan distance between the entrance and the exit is `|x_1 - x_2| + |y_1 - y_2| = |0-9| + |0-9| = 18`.\n",
    "\n",
    "**Q. Try to understand and explain why such distance makes the heuristic admissible (you should primarily answer why it never over-estimates the remaining cost to reach the goal). Try to find another admissible heuristic and a non-admissible one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cnp5O0tFROnB"
   },
   "source": [
    "**Q. By filling the gaps in the code below, create a function that takes as input a node in the labyrinth graph (identified by its pair of indices) and returns the Manhattan distance between that node and the exit.**\n",
    "\n",
    "This will be our $h(n)$ in our A* algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c91wz1AxjEy"
   },
   "outputs": [],
   "source": [
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-i_XI0WVCMv"
   },
   "outputs": [],
   "source": [
    "# An example usage should be h([0,0]) = 18, and h([9,9]) = 0.\n",
    "\n",
    "def heuristic(node): # Calculates the admissible heuristic of a node\n",
    "    # I know the format is [X,Y]\n",
    "    node = node.replace('[','') # remove brackets\n",
    "    node = node.replace(']','')\n",
    "    x, y = node.split(',', maxsplit=2) # Split values by ,\n",
    "    x = float(x)\n",
    "    y = float(y)\n",
    "    # TO DO\n",
    "    return abs(x-9) + abs(y-9) # Return calculation of admissible heuristic (manhattan distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyWLMSLRjPFi"
   },
   "source": [
    "You are now ready to implement your A* algoirithm. If you need a reminder, A* is just the UCS, except that the cost of each node is $f(n)=g(n)+h(n)$, as opposed to only $g(n)$, where $g(n)$ is the path-cost, and $h(n)$ is the heuristic cost.\n",
    "\n",
    "**Q. By filling the gaps in the code below, implement the A-star algorithm and find the best-path in our labyrinth.**\n",
    "\n",
    "\n",
    "HINT: If you are stuck, have a look at these references:\n",
    "\n",
    "- [search.py](https://github.com/aimacode/aima-python/blob/master/search.py): python implementation of all the search algorithms from chapter 3 of the book.\n",
    "\n",
    "- [search.ipynb](https://github.com/aimacode/aima-python/blob/master/search.ipynb): a notebook demonstration of those search algorithm.\n",
    "\n",
    "- [search4e.ipynb](https://github.com/aimacode/aima-python/blob/master/search4e.ipynb): an updated self-containing notebook with a slightly more concise (and sometimes improved) implementation of the same search algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pLZshJfypXh"
   },
   "outputs": [],
   "source": [
    "def Astar(graph, origin, goal):\n",
    "    admissible_heuristics = {} # Will save the values of h so i don't need to calculate multiple times for every node\n",
    "    h = heuristic(origin)\n",
    "    admissible_heuristics[origin] = h\n",
    "    visited_nodes = {} # This will contain the data of how to get to any node\n",
    "    visited_nodes[origin] = (h, [origin]) # I add the data for the origin node: \"Travel cost + heuristic\", \"Path to get there\" and \"Admissible Heuristic\"\n",
    "\n",
    "    # TO DO\n",
    "    paths_to_explore = PriorityQueue()\n",
    "    paths_to_explore.put((h, [origin], 0)) # Add the origin node to paths to explore, also add cost without h\n",
    "    # I add the total cost, as well as the path to get there (they will be sorted automatically)\n",
    "\n",
    "    while not paths_to_explore.empty(): # While there are still paths to explore\n",
    "        # Pop elemenet with lower path cost in the queue\n",
    "        _, path, total_cost = paths_to_explore.get()\n",
    "        current_node = path[-1]\n",
    "        if current_node == goal:\n",
    "            return visited_nodes[goal]\n",
    "        neighbors = graph.neighbors(current_node) # I get all the neighbors of the current path\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            edge_data = graph.get_edge_data(path[-1], neighbor)\n",
    "            if \"weight\" in edge_data:\n",
    "                cost_to_neighbor = edge_data[\"weight\"] # If the graph has weights\n",
    "            else:\n",
    "                cost_to_neighbor = 1 # If the graph does not have weights I use 1\n",
    "\n",
    "            if neighbor in admissible_heuristics:\n",
    "                h = admissible_heuristics[neighbor]\n",
    "            else:\n",
    "                h = heuristic(neighbor)\n",
    "                admissible_heuristics[neighbor] = h\n",
    "\n",
    "            new_cost = total_cost + cost_to_neighbor\n",
    "            new_cost_plus_h = new_cost + h\n",
    "            if (neighbor not in visited_nodes) or (visited_nodes[neighbor][0]>new_cost_plus_h): # If this node was never explored, or the cost to get there is better than te previous ones\n",
    "                next_node = (new_cost_plus_h, path+[neighbor], new_cost)\n",
    "                visited_nodes[neighbor] = next_node # Update the node with best value\n",
    "                paths_to_explore.put(next_node) # Also will add it as a possible path to explore\n",
    "\n",
    "\n",
    "    return visited_nodes[goal] # I will return the goal information, it will have both the total cost and the path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4uII8tNldqB"
   },
   "outputs": [],
   "source": [
    "#  create the graph from the json file\n",
    "labyrinth = load_graph_from_file(\"middle_labyrinth.json\")\n",
    "solution = Astar(labyrinth,\"[0,0]\",\"[9,9]\")\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfMvyBHmROnD"
   },
   "source": [
    "---\n",
    "# Local Search with Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0vFHoZ1ljST"
   },
   "source": [
    "Simulated annealing is an empirical optimization method. As its name suggests, it makes the analogy between the way metals are handled by different firings to obtain a crystalline structure of minimum energy and the search for a **minimum of a cost function**. In a simple version, the algorithm can be considered as follows when looking for the minimum of a function $f$:\n",
    "\n",
    "1. **Initialization**: The starting point is a point $x_0$ and a temperature $T_0$.\n",
    "2. **Visiting a neighbour**: We randomly move to visit a neighbour of the current state $x$, defined as $x_{t+1} = x_t + D$. $D$ is a random variable (from a Gaussian) of mean zero and variance $\\kappa e^{-1/(1000\\times T)}$ where $\\kappa$ is a parameter.\n",
    "3. **Heat evaluation**: We evaluate $f(x_{t+1})$. If the solution is better (i.e $f(x_{t+1})$ < $f(x_t)$ ) we keep it **OTHERWISE** we keep it with a probability of$\\kappa' e^{-1/(1000\\times T)}$, where $\\kappa'$ is another parameter.\n",
    "4. **The annealing schedule**: We decrease the temperature $T$.\n",
    "\n",
    "\n",
    "Note that the stop criterion can be a threshold stating whether the solution is acceptable or a number of maximum iterations.\n",
    "\n",
    "## Snowboarding\n",
    "\n",
    "A ski resort wants to open a new extreme slope for next year. So it calls in a landscape architect who is in charge of designing it. It comes back to you later on with a simple mathematical formula modeling, at a scale of 1/1000, the slope of his dream:\n",
    "$$f(x) = x^4 − x^3 − 20x^2 + x + 1$$\n",
    "\n",
    "Using the following function, you decide to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhX78LKUj9pk"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "[random.random() for T in range(10,0,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-kkEXmHwYDF"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "# Function allowing to have a nice plot of function\n",
    "def visualize_function(x_values, y_values, z_values, x_label, y_label, z_label, title):\n",
    "  fig = plt.figure() # opens a figure environment\n",
    "  ax = fig.add_subplot(projection='3d') # to perform a 3D plot\n",
    "  surf = ax.plot_surface(x_values, y_values, z_values, rstride=1, cstride=1, linewidth=0, antialiased=False , cmap = cm.jet) #plot definition and options\n",
    "  ax.set_xlabel(x_label)\n",
    "  ax.set_ylabel(y_label)\n",
    "  ax.set_zlabel(z_label)\n",
    "  plt.title(title)\n",
    "  plt.show()\n",
    "\n",
    "X = np.arange(-5, 5, 0.05) # values for X\n",
    "Y = np.arange(-5, 5, 0.05) # values for X\n",
    "X, X = np.meshgrid(X, X) # creates a squared grid on which to plot the function values (Z)\n",
    "Z = X**4 - X**3 - 20*X**2 + X +1 # defines the function values\n",
    "\n",
    "visualize_function(X, Y, Z, \"x\", \"x\", \"f(x)\", \"Slope design by the architect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rgvRhG8zW6X"
   },
   "source": [
    "When looking at it, you just realise that it is indeed quite extreme. Apparently the architect likes steep slopes. However, your goal is to make it enjoyable for more people. In order to do so, you want to install snow cannons a bit everywhere to smooth it out. Unfortunately, the designer had a pretty bad accident skiing and cannot tell you the most crucial point: where are the deepest cavities. Therefore, you decide to find where it is by implementing the simulated annealing.\n",
    "\n",
    "**Q. By filling the gaps below, create a function taking as input the initial point ($x_0$), the two parameters ($\\kappa$ and $\\kappa'$), $t_0$ and $t_{max}$. The last two parameters determine the bound of the minimal and maximal temperature with $T_{max} = 1 / t_0$ and $T_{min} = 1 / t_{max}$ with $t$ being incremented by step of `1`. Your function should return two lists. The first one contains all the values $x$ that you visited and the other one the corresponding $f(x)$. Use the following parameters: $x_0 = 0$, $\\kappa = 10$, $\\kappa' = 0.5$, $t_{min} = 1$ and $t_{max} = 1000$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kraNSyRG5Ufw"
   },
   "outputs": [],
   "source": [
    "# Your function for the simulated annealing\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def get_estimate(k, T):\n",
    "    return k * math.exp(-1/(1000*T))\n",
    "\n",
    "def function(X):\n",
    "    Y = X**4 - X**3 - 20*X**2 + X +1 # defines the function values\n",
    "    return Y\n",
    "\n",
    "# Initial parameters\n",
    "T0 = 1000 # Initial Temperature\n",
    "# You may want to compare to k=10 as well\n",
    "k = noise_amplitude = 0.6\n",
    "X = 0 # Initial point\n",
    "k_prime = 0.5\n",
    "\n",
    "Y = function(X)\n",
    "\n",
    "for T in range(T0,0,-1): # Will go from T0 to 0\n",
    "    # TO DO\n",
    "    change_probabilty = get_estimate(k_prime, T)\n",
    "    variance = get_estimate(k, T)\n",
    "    std_deviation = np.sqrt(variance)\n",
    "    D = np.random.normal(0, std_deviation)\n",
    "    # Noise could be added in a different way as well\n",
    "    # D = random.random() * 2 * noise_amplitude - noise_amplitude # We use perturbation as uniform distribution.  We can reduce the perturbation as a function of temperature\n",
    "    X1 = X + D\n",
    "    Y1 = function(X1)\n",
    "    if (Y1 < Y) or (change_probabilty < random.random()): # If we accept the change (either better result or accepting by chance)\n",
    "        Y = Y1 # We update the values\n",
    "        X = X1\n",
    "# We will find the correct result (around 3) 50% of the times\n",
    "# To improve we need better parameters, a more sophisticated algorithm or run it many times\n",
    "print(\"Found X: \" + str(X) + \". Y: \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq0eWgb55dui"
   },
   "source": [
    "**Q. What is the global minimum found by your algorithm? If you run 10 times your function, would you get always the same result? Why? How many values have you visited before finding the global minimum?**\n",
    "\n",
    "**Q. Play with the values of $\\kappa$, $\\kappa'$ and $t_{max}$ to fully understand their impact. Modify the way the temperature changes with another function (e.g., exponential decay schedule).**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
